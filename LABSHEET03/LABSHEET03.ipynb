{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8ab4a0-68d3-47de-9b08-2c6f0c7a98ab",
   "metadata": {},
   "source": [
    "### AUTHOR: VAISHNAV KRISHNA P\n",
    "#### SUBJECT: NATURAL LANGUAGE PREPROCESSING CSE 3188\n",
    "#### LABSHEET03: STOPWORDS, AUTOCORRECT, STEMMING, LEMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc505c11-56b2-42b1-bd28-6b3cbe1792fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some neesssory libraries \n",
    "import nltk \n",
    "from nltk import download\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e8fd2-6f7e-4c60-ba0e-91cc735fc24a",
   "metadata": {},
   "source": [
    "#### TOKENIZATION AND POS_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4d3289-37a0-4227-a467-1260cc0fa56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'fundamentals', '.']\n"
     ]
    }
   ],
   "source": [
    "# text \n",
    "text = \"I am reading NLP fundamentals.\"\n",
    "\n",
    "# word tokenization\n",
    "def tokenization(text):\n",
    "    print(word_tokenize(text))\n",
    "tokenization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c907fe4c-1c94-43ee-85a1-4c806dadf33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('reading', 'VBG'), ('NLP', 'NNP'), ('fundamentals', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# applying the parts-of-speech \n",
    "def apply_pos(text):\n",
    "    print(pos_tag(word_tokenize(text)))\n",
    "apply_pos(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ec834-2f67-44e8-a552-e5f3b3171cdb",
   "metadata": {},
   "source": [
    "#### STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1143db28-f1a8-4f2c-bb7f-287af0a23b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# English stopwords \n",
    "print(stopwords.words('english')) # <- a list of words which does not contribute too much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6dca7ce-5fe6-4abb-bd83-2128bc2ad2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No of stopwords \n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc797ed6-d2b7-40a7-8334-26affa4ac6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stopwords from the text \n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    clean_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "271f8db8-8dc4-4f1d-ae1f-4f93784b65ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reading nlp fundamentals .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89981bde-321a-433a-a85d-e05d74de273e",
   "metadata": {},
   "source": [
    "#### AUTOCORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f20c1e28-4b35-469d-b0bb-8354a2d44fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model for autocorrect \n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64d51a7a-2147-469d-879a-2b203c8c57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object \n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b65829b-70ee-4ae7-ad62-4b4bad5a737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English text \n",
    "text = '''Ntural languag processin deals the art of extracting insightes from Natural languags'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07b59b89-f73c-4336-b91f-9c8d7acb975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    words = text.split(\" \")\n",
    "    corrected_words = [spell(word) for word in words]\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4983024c-abf5-4189-b8c5-24a32778cb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing deals the art of extracting insights from Natural language'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling function\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08704216-c5de-471c-b40a-44b210311670",
   "metadata": {},
   "source": [
    "#### STRING REPLACING FUNCTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7925455-ef40-4044-a358-23b97d77979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text \n",
    "text = \"I visited US from the UK on 22-10-18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364acc7-4349-4e89-89a8-164bfb01cc0f",
   "metadata": {},
   "source": [
    "Note : \n",
    "- replace US -> United State \n",
    "- replace UK -> United Kingdom\n",
    "- replace 18 -> 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bacd5a8-c23b-43d5-b066-d17c46dffc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I visited United State from the United Kingdom on 22-10-2018'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"US\",\"United State\").replace(\"UK\", \"United Kingdom\").replace(\"18\",\"2018\")\n",
    "    return text\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ab5bf-f05e-42be-882d-4ec727e6caa2",
   "metadata": {},
   "source": [
    "#### STEMMING\n",
    "- Used to optain the root of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0019402-7469-4841-b229-d5bd035a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the lobraries \n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "400c473a-398b-49d0-b11c-1f13583cf7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the class \n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b80b3f1-6594-4a78-b0c7-33258e6f2eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['come', 'fire', 'eat', 'bat']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appliying the stemming\n",
    "words = [\"coming\", 'firing', 'eating', 'batting']\n",
    "\n",
    "def apply_stemming(words):\n",
    "    words = [stemming.stem(word) for word in words]\n",
    "    return words\n",
    "apply_stemming(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5fad9-7da9-47a8-bff0-48db01e8a5ac",
   "metadata": {},
   "source": [
    "#### LEMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f4ce111-97c3-45a7-bc1f-b47640390db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the lematizer \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f64b6d19-870b-4a33-9116-da2ea936ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object \n",
    "lematizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "597c2def-dacf-4447-8472-39ec78f14949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['come', 'fire', 'eat', 'run']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the lematizing function\n",
    "words = [\"coming\", 'firing', 'eating', 'running']\n",
    "\n",
    "def apply_lematization(words):\n",
    "    words = [lematizer.lemmatize(word , pos='v')  for word in words]\n",
    "    return words\n",
    "apply_lematization(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
